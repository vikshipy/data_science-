{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb4454fb-6e4f-420f-9391-3338904a7380",
   "metadata": {},
   "source": [
    "Q1. What is boosting in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a780d6c8-3161-446d-bf57-e8a6d4d51673",
   "metadata": {},
   "source": [
    "####\n",
    "\n",
    "\n",
    "Boosting is a machine learning technique that combines multiple weak or base learners to create a strong predictive model. It is a sequential process where each base learner is trained to correct the mistakes made by the previous ones. The goal of boosting is to improve the overall performance of the model by reducing bias and variance.\n",
    "\n",
    "Boosting works by assigning weights to each training example in the dataset. Initially, all examples are given equal weight, and a base learner is trained on the data. After each iteration, the weights are readjusted based on the errors made by the previous learner. Examples that were incorrectly classified receive higher weights, making them more likely to be correctly classified in the next iteration. This process continues for a specified number of iterations or until a desired level of accuracy is achieved.\n",
    "\n",
    "The final prediction of the boosting model is a weighted combination of the predictions made by each base learner. Generally, base learners are simple models like decision trees, also known as weak learners, as they have low predictive power individually. However, when combined through boosting, they can create a highly accurate and robust model.\n",
    "\n",
    "Popular boosting algorithms include AdaBoost (Adaptive Boosting), Gradient Boosting, and XGBoost (Extreme Gradient Boosting). These algorithms differ in how they update the weights, combine the weak learners, and handle the training process. Boosting is widely used in various domains, including classification, regression, and ranking tasks, due to its ability to effectively leverage multiple weak models and generate strong predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad57a6a-cd51-4c2d-bce5-d9888327afc0",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "Q2. What are the advantages and limitations of using boosting techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3708f7-60d2-4dfb-ac15-4ecb469d045f",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "Boosting techniques offer several advantages in machine learning:\n",
    "\n",
    "1. Improved Accuracy: Boosting can significantly enhance the accuracy of predictive models compared to using individual weak learners. By iteratively focusing on examples that are challenging to classify, boosting reduces bias and variance, resulting in better overall performance.\n",
    "\n",
    "2. Versatility: Boosting can be applied to various types of machine learning tasks, including classification, regression, and ranking. It is not restricted to any specific problem domain and can adapt to different data types and structures.\n",
    "\n",
    "3. Handling Complex Relationships: Boosting algorithms can capture complex relationships and interactions between features in the data. The sequential nature of boosting allows subsequent weak learners to focus on correcting the errors made by previous learners, improving the model's ability to handle complex patterns.\n",
    "\n",
    "4. Feature Importance: Boosting algorithms can provide insights into feature importance. By observing the frequency with which features are used in the boosting process, one can identify the most relevant features for making predictions.\n",
    "\n",
    "However, boosting techniques also have some limitations:\n",
    "\n",
    "1. Sensitivity to Noisy Data: Boosting is sensitive to noisy data and outliers. As it assigns higher weights to misclassified examples, outliers can have a strong influence on the model's learning process, potentially leading to overfitting.\n",
    "\n",
    "2. Computationally Intensive: Boosting requires training multiple iterations of weak learners, which can be computationally intensive and time-consuming, especially when dealing with large datasets or complex models. The training process may take longer compared to simpler algorithms.\n",
    "\n",
    "3. Overfitting: If not properly controlled, boosting can lead to overfitting, especially when the model becomes too complex or the number of iterations is high. Regularization techniques, such as limiting the tree depth or using shrinkage parameters, can help mitigate this issue.\n",
    "\n",
    "4. Lack of Interpretability: Boosting models are generally considered as black boxes, making it challenging to interpret the inner workings and decision-making process. While feature importance can be derived, understanding the underlying rationale for specific predictions can be difficult.\n",
    "\n",
    "It is important to consider these advantages and limitations when deciding whether to use boosting techniques in a specific machine learning task and to carefully tune the parameters to achieve the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3034f3-19b3-4421-849e-41175f8b0aed",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "Q3. Explain how boosting works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0696e2-9da7-43e7-a62f-bc39afb52e8c",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "Boosting is a machine learning technique that combines multiple weak learners (also known as base learners) to create a strong predictive model. The main idea behind boosting is to iteratively train weak learners and assign higher weights to examples that are difficult to classify correctly. This iterative process helps improve the overall performance of the model by reducing bias and variance.\n",
    "\n",
    "Here is a step-by-step explanation of how boosting works:\n",
    "\n",
    "1. Initialization: All training examples are assigned equal weights. Initially, each example has a weight of 1/N, where N is the total number of training examples.\n",
    "\n",
    "2. Training Weak Learners: The first weak learner (often a decision tree) is trained on the training data using the initial weights. It aims to minimize the training error by finding the best split points and thresholds to separate the data into different classes or predict continuous values.\n",
    "\n",
    "3. Weight Update: After the weak learner is trained, the weights of the training examples are updated. Examples that were misclassified by the weak learner receive higher weights, making them more influential in the subsequent iterations. The weights are adjusted based on a predefined weight update formula.\n",
    "\n",
    "4. Iterative Process: Steps 2 and 3 are repeated iteratively for a specified number of iterations or until a stopping condition is met. In each iteration, a new weak learner is trained on the updated weights, and the weights are further adjusted based on the errors made by the previous learners.\n",
    "\n",
    "5. Combining Weak Learners: The final prediction of the boosting model is a weighted combination of the predictions made by each weak learner. Typically, the predictions of weak learners with lower errors have higher weights in the final prediction.\n",
    "\n",
    "6. Prediction: To make predictions on new, unseen data, the weak learners are applied sequentially to the input features, and their predictions are combined using the weights learned during the training phase.\n",
    "\n",
    "The overall effect of boosting is that subsequent weak learners focus more on the examples that were previously misclassified or are more difficult to classify correctly. By iteratively adjusting the weights and combining the predictions of weak learners, boosting effectively adapts and learns from the mistakes of previous learners, leading to improved accuracy and a stronger overall model.\n",
    "\n",
    "Popular boosting algorithms include AdaBoost (Adaptive Boosting), Gradient Boosting, and XGBoost (Extreme Gradient Boosting), each with its own specific variations and enhancements to the boosting process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239e7655-eda6-4b66-9add-fbaf01c2b0f6",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "Q4. What are the different types of boosting algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87757a8-574f-49f6-bd2c-a09a281f7824",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "There are several different types of boosting algorithms that have been developed over the years. Here are some of the most popular ones:\n",
    "\n",
    "1. AdaBoost (Adaptive Boosting): AdaBoost is one of the earliest and most well-known boosting algorithms. It assigns weights to training examples and trains a series of weak learners on the data. Each weak learner is trained to correct the mistakes made by the previous ones. AdaBoost increases the weights of misclassified examples and reduces the weights of correctly classified examples, allowing subsequent weak learners to focus on the challenging examples.\n",
    "\n",
    "2. Gradient Boosting: Gradient Boosting is a general framework that can be used with various loss functions and weak learner types. It works by sequentially adding weak learners to the model, with each learner trained to minimize the residual errors of the previous learners. The new learner is fitted to the negative gradient of the loss function, hence the name \"Gradient Boosting.\" Gradient Boosting includes popular implementations like Gradient Boosting Machines (GBM) and the more advanced XGBoost and LightGBM algorithms.\n",
    "\n",
    "3. XGBoost (Extreme Gradient Boosting): XGBoost is an optimized implementation of Gradient Boosting that has gained significant popularity due to its efficiency and performance. It incorporates additional enhancements such as regularization techniques, parallel processing, and tree pruning, making it faster and more accurate than traditional Gradient Boosting. XGBoost is widely used in various machine learning competitions and real-world applications.\n",
    "\n",
    "4. LightGBM: LightGBM is another high-performance gradient boosting framework that is known for its efficiency and scalability. It introduces the concept of Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB) to speed up the training process. LightGBM is particularly effective for large-scale datasets and high-dimensional feature spaces.\n",
    "\n",
    "5. CatBoost: CatBoost is a boosting algorithm developed by Yandex. It is specifically designed to handle categorical features efficiently. CatBoost employs various techniques like gradient-based optimization, ordered boosting, and random permutations to improve the quality of predictions. It also includes built-in handling of missing values and robust handling of categorical variables without the need for extensive preprocessing.\n",
    "\n",
    "These are just a few examples of boosting algorithms. Each algorithm has its own strengths, variations, and optimizations, making them suitable for different types of problems and data characteristics. Choosing the right boosting algorithm depends on factors such as the nature of the data, computational requirements, and specific performance objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390fc967-57a6-4a77-b536-91b5206fe32f",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "Q5. What are some common parameters in boosting algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efaf6d4-6eb7-4a3f-a7d5-96e1aaf1d047",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "Boosting algorithms have various parameters that can be tuned to optimize their performance. Here are some common parameters found in boosting algorithms:\n",
    "\n",
    "1. Number of Iterations: This parameter determines the number of weak learners (iterations) that will be sequentially trained. Increasing the number of iterations can improve the model's performance, but it may also increase the risk of overfitting.\n",
    "\n",
    "2. Learning Rate (or Shrinkage): The learning rate controls the contribution of each weak learner to the final model. A lower learning rate means that each weak learner has less influence, reducing the risk of overfitting. However, a lower learning rate may require more iterations to achieve good performance.\n",
    "\n",
    "3. Base Learner Type: Boosting algorithms typically use a specific type of weak learner, such as decision trees or linear models. The parameters associated with the base learner, such as the maximum tree depth or regularization parameters, can impact the overall performance of the boosting algorithm.\n",
    "\n",
    "4. Subsampling (or Stochastic Gradient Boosting): Some boosting algorithms allow subsampling, where a fraction of the training data is randomly selected for each iteration. Subsampling can improve the training speed and reduce memory usage, but it may introduce additional randomness into the learning process.\n",
    "\n",
    "5. Regularization Parameters: Boosting algorithms may include regularization techniques to prevent overfitting. These parameters control the complexity of the weak learners, such as the maximum depth of decision trees or the strength of regularization terms in linear models.\n",
    "\n",
    "6. Loss Function: The choice of loss function depends on the specific problem being addressed (e.g., binary classification, regression). Different boosting algorithms offer various loss functions, such as exponential loss for AdaBoost or mean squared error for gradient boosting.\n",
    "\n",
    "7. Feature Importance Calculation: Boosting algorithms often provide measures of feature importance, which indicate the relevance or contribution of each feature in the model. Parameters related to feature importance calculation, such as the method or threshold, may be available.\n",
    "\n",
    "8. Parallel Processing: Many boosting implementations offer options for parallel processing, allowing the algorithm to leverage multiple CPU cores or distributed computing frameworks to speed up training.\n",
    "\n",
    "These parameters may vary across different boosting algorithms and implementations. Tuning these parameters through techniques like grid search or random search can help optimize the performance of the boosting model for a specific task. It's important to note that the optimal parameter values depend on the dataset, problem complexity, and available computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d15a11-bd81-4d7a-b489-6bf1498e0dd6",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "Q6. How do boosting algorithms combine weak learners to create a strong learner?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abac0261-25ce-484a-92ea-fe887a29fec3",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "Boosting algorithms combine weak learners to create a strong learner through a process of weighted voting or weighted averaging. The specific mechanism varies depending on the boosting algorithm, but the general idea is to give more weight to the predictions of the more accurate weak learners while downweighting or ignoring the predictions of the weaker ones. Here are two common approaches used by boosting algorithms:\n",
    "\n",
    "1. Weighted Voting: In this approach, each weak learner is assigned a weight based on its performance or accuracy. The weights reflect the contribution of each weak learner to the final prediction. Weak learners with higher accuracy have higher weights, indicating that their predictions are more reliable. The final prediction is obtained by combining the predictions of all weak learners, weighted by their respective weights. This can be done through majority voting (for classification problems) or weighted averaging (for regression problems).\n",
    "\n",
    "2. Additive Combination: In this approach, each weak learner is trained to correct the mistakes made by the previous learners. The weak learners are added sequentially to the model, and each new learner is fitted to the residual errors (the differences between the target values and the predictions made by the previous learners). The predictions of all weak learners are then added together to obtain the final prediction. The sequential addition of weak learners gradually improves the overall prediction accuracy, reducing the errors made by the previous learners.\n",
    "\n",
    "It's important to note that the specific mechanism of combining weak learners may differ among different boosting algorithms. Some algorithms, like AdaBoost, focus on updating the weights of training examples and combining predictions through weighted voting. Others, like gradient boosting and XGBoost, follow the additive combination approach, where weak learners are trained on the residuals and their predictions are added together.\n",
    "\n",
    "The overall goal of combining weak learners is to create a strong learner that is more accurate and robust than the individual weak learners. By leveraging the strengths of multiple weak learners and addressing their collective weaknesses, boosting algorithms aim to achieve better predictive performance and generalize well to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa2ceaa-1aaf-4960-a1af-bc36c1d3638f",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "Q7. Explain the concept of AdaBoost algorithm and its working."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc2e5ac-15a9-4a29-8993-2202ee284a2d",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "AdaBoost (Adaptive Boosting) is a popular boosting algorithm that combines multiple weak learners to create a strong learner. It was proposed by Yoav Freund and Robert Schapire in 1996. The key idea behind AdaBoost is to iteratively train weak learners on the data, adjusting the weights of the training examples to focus on the challenging examples that were previously misclassified.\n",
    "\n",
    "Here is an overview of how AdaBoost works:\n",
    "\n",
    "1. Initialization: All training examples are assigned equal weights, usually set to 1/N, where N is the total number of training examples.\n",
    "\n",
    "2. Training Weak Learners: The first weak learner, often a decision stump (a decision tree with a single split), is trained on the weighted training data. The weak learner aims to minimize the weighted training error, where the weights reflect the importance of each example. The weighted error is calculated as the sum of weights of misclassified examples divided by the sum of all weights.\n",
    "\n",
    "3. Weight Update: After the weak learner is trained, the weights of the training examples are updated. Examples that were misclassified receive higher weights, making them more influential in the subsequent iterations. The weight update formula gives more weight to examples that were misclassified, encouraging the next weak learner to focus on these challenging examples.\n",
    "\n",
    "4. Iterative Process: Steps 2 and 3 are repeated for a specified number of iterations or until a stopping condition is met. In each iteration, a new weak learner is trained on the updated weights, and the weights are further adjusted based on the errors made by the previous learners. Each new weak learner is trained to correct the mistakes made by the previous learners, with a focus on the examples that were difficult to classify.\n",
    "\n",
    "5. Combining Weak Learners: The final prediction of the AdaBoost model is obtained by combining the predictions of all weak learners. The contributions of individual weak learners are weighted based on their accuracy during training. Weak learners with higher accuracy have higher weights, indicating that their predictions are more reliable. The combined predictions can be obtained through weighted voting (for classification) or weighted averaging (for regression).\n",
    "\n",
    "The AdaBoost algorithm places more emphasis on examples that were previously misclassified by giving them higher weights. By iteratively training weak learners and updating the weights, AdaBoost effectively adapts to the difficult examples and focuses on learning from its mistakes. The final model is a weighted combination of the weak learners, where the accuracy of each learner influences its contribution to the final prediction.\n",
    "\n",
    "AdaBoost is known for its ability to handle complex datasets and improve classification accuracy. It has been widely used in various applications, such as face detection, object recognition, and bioinformatics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d221e66-9b9e-4298-a734-7fbedf17e8ad",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "Q8. What is the loss function used in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f183606f-fbf2-4f5c-81f1-0537eb603fa7",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "The AdaBoost algorithm does not directly optimize a specific loss function. Instead, it uses a variant of the exponential loss function, often referred to as the AdaBoost loss or the exponential loss.\n",
    "\n",
    "The exponential loss function used in AdaBoost is defined as:\n",
    "\n",
    "L(y, f(x)) = exp(-y * f(x))\n",
    "\n",
    "where:\n",
    "- L is the exponential loss function\n",
    "- y is the true label of the example (usually +1 or -1 for binary classification)\n",
    "- f(x) is the predicted score or output of the AdaBoost model for the example x\n",
    "\n",
    "The exponential loss assigns higher penalties for misclassified examples, with the penalty increasing exponentially as the predicted score (y * f(x)) moves away from the correct label. In other words, it amplifies the impact of misclassifications and encourages the boosting algorithm to focus on improving the classification of challenging examples.\n",
    "\n",
    "In each iteration of AdaBoost, the weak learner is trained to minimize the weighted exponential loss. The weights of the training examples are adjusted to give higher importance to the misclassified examples from the previous iteration, making them more influential in the subsequent training. The goal is to find the weak learner that minimizes the weighted exponential loss, which leads to a higher accuracy in the subsequent iterations.\n",
    "\n",
    "While the exponential loss is commonly used in AdaBoost, it's worth noting that other loss functions, such as the binomial deviance or logistic loss, can also be used in boosting algorithms, depending on the specific implementation or problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb34aa2e-5b2e-4719-8f05-e54205b74c0a",
   "metadata": {},
   "source": [
    "####\n",
    "\n",
    "Q9. How does the AdaBoost algorithm update the weights of misclassified samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c65fbe-3ffe-4ab0-81cc-d0eb1fa37109",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "In the AdaBoost algorithm, the weights of the training examples are updated after each iteration to give higher importance to the misclassified examples. The weight update process in AdaBoost is based on the concept of error rate and exponential loss. Here's how the weights of misclassified samples are updated in AdaBoost:\n",
    "\n",
    "1. Initialization: All training examples are assigned equal weights, usually set to 1/N, where N is the total number of training examples.\n",
    "\n",
    "2. Training Weak Learner: A weak learner, such as a decision stump, is trained on the weighted training data.\n",
    "\n",
    "3. Error Calculation: After the weak learner is trained, the error rate (ε) of the weak learner is computed. The error rate is calculated as the sum of the weights of the misclassified examples divided by the sum of all weights.\n",
    "\n",
    "4. Weight Update: The weights of the training examples are updated based on the error rate. The weight update formula used in AdaBoost is as follows:\n",
    "\n",
    "   For each training example i:\n",
    "      - If example i is misclassified:\n",
    "          wi = wi * exp(α)\n",
    "      - If example i is correctly classified:\n",
    "          wi = wi * exp(-α)\n",
    "\n",
    "   Here, α (alpha) is a weight factor determined by the error rate. It is calculated as:\n",
    "   α = 0.5 * ln((1 - ε) / ε)\n",
    "\n",
    "   The weight update formula increases the weights of the misclassified examples (wi * exp(α)), making them more influential in the subsequent iterations. On the other hand, it decreases the weights of the correctly classified examples (wi * exp(-α)), reducing their influence.\n",
    "\n",
    "5. Weight Normalization: After updating the weights, they are normalized to ensure that they sum up to 1. This normalization step helps maintain the relative importance of the examples.\n",
    "\n",
    "By updating the weights of misclassified examples, AdaBoost effectively directs the subsequent weak learners to focus on these challenging examples. The weight update formula ensures that the weak learners in later iterations pay more attention to the misclassified examples, allowing the boosting algorithm to learn from its mistakes and improve the overall accuracy of the model.\n",
    "\n",
    "This process of updating the weights and training weak learners is repeated for a specified number of iterations, resulting in a final ensemble model that combines the predictions of all the weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da4bb22-ee25-456b-a101-bfef6466184f",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca69629b-da77-4925-8ec1-6e31500eba0b",
   "metadata": {},
   "source": [
    "##\n",
    "\n",
    "Increasing the number of estimators in the AdaBoost algorithm can have several effects on the model's performance. Here are some key effects:\n",
    "\n",
    "1. Improved Training Accuracy: As the number of estimators increases, the AdaBoost model has more opportunities to learn from the training data. Each additional estimator contributes to refining the model's ability to capture complex patterns and make accurate predictions. Consequently, increasing the number of estimators tends to improve the training accuracy of the AdaBoost model.\n",
    "\n",
    "2. Potential Overfitting: Although increasing the number of estimators can improve the model's accuracy on the training data, there is a risk of overfitting. Overfitting occurs when the model becomes too specialized in the training data, resulting in poor generalization to unseen data. Adding more estimators may cause the model to memorize the training examples, leading to decreased performance on new data.\n",
    "\n",
    "3. Longer Training Time: As the number of estimators increases, the training time of the AdaBoost algorithm also increases. Each additional estimator requires training, which may involve constructing decision trees or training other weak learners. Consequently, training a larger ensemble of estimators can be computationally more expensive and time-consuming.\n",
    "\n",
    "4. Smoother Decision Boundaries: AdaBoost combines multiple weak learners to create a strong learner. With more estimators, the decision boundaries of the model tend to become smoother and more refined. The ensemble can better capture intricate decision regions and improve the separation of different classes, leading to enhanced generalization performance.\n",
    "\n",
    "5. More Robust Predictions: Increasing the number of estimators in AdaBoost can enhance the model's robustness to noise or outliers in the data. The ensemble approach helps mitigate the impact of individual weak learners that may be sensitive to noisy or erroneous data points. By combining the predictions of multiple estimators, AdaBoost becomes more resilient to outliers and improves the stability of the overall predictions.\n",
    "\n",
    "It's important to find the right balance when determining the number of estimators in AdaBoost. Adding more estimators can lead to better performance up to a certain point, but beyond that, it may result in diminishing returns or even overfitting. Cross-validation or other model evaluation techniques can be used to select the optimal number of estimators for a given problem, finding the trade-off between performance and computational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dfb016-aeed-4445-bf1e-f4248c15b76a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e6f5f4-3a7d-45c4-b9e5-7273384a18ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb04572-17b0-487d-9874-4de93a5e4cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2affa1dc-9d6e-4fca-87a1-611bef8a48ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb7fb01-06e5-4cd2-80fe-3ff65542c2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
