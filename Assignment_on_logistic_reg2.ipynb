{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2f7df7a-a99c-4128-8427-51e1348a581e",
   "metadata": {},
   "source": [
    "Q1. What is the purpose of grid search cv in machine learning, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffe098a-8b63-4cf2-8691-4be30f4cd354",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "The purpose of GridSearchCV (Grid Search with Cross-Validation) in machine learning is to find the optimal combination of hyperparameters for a given model. Hyperparameters are configuration settings that are set before training a model and cannot be learned from the data. GridSearchCV automates the process of systematically searching through a specified hyperparameter grid and evaluating model performance using cross-validation.\n",
    "\n",
    "Here's how GridSearchCV works:\n",
    "\n",
    "1. Define the Hyperparameter Grid: Specify a dictionary or a list of dictionaries that defines the hyperparameters and the corresponding values to be explored. Each dictionary represents a set of hyperparameters and their potential values. For example, if you have a decision tree classifier, you can create a grid with hyperparameters like max_depth and min_samples_split and their respective values.\n",
    "\n",
    "2. Define the Model and Scoring Metric: Select the machine learning model to be tuned and choose an appropriate scoring metric to evaluate the model's performance. The scoring metric could be accuracy, F1 score, precision, recall, or any other suitable measure based on the problem at hand.\n",
    "\n",
    "3. Perform Cross-Validation: Split the training data into multiple folds (subsets) for cross-validation. For each combination of hyperparameters, the model is trained on a subset of the data and evaluated on the remaining fold. This process is repeated for each fold, and the performance metric is calculated and averaged across all folds.\n",
    "\n",
    "4. Find the Best Hyperparameters: GridSearchCV keeps track of the performance metrics for each combination of hyperparameters. Once the cross-validation process is complete, it identifies the combination of hyperparameters that resulted in the best performance based on the chosen evaluation metric.\n",
    "\n",
    "5. Retrain the Model: After determining the best hyperparameters, the model is retrained using the entire training dataset with the optimal hyperparameters.\n",
    "\n",
    "6. Evaluate the Model: Finally, the performance of the model with the best hyperparameters is assessed on the test dataset or an independent validation set to estimate its generalization performance.\n",
    "\n",
    "GridSearchCV simplifies the process of hyperparameter tuning by systematically exploring the hyperparameter space and evaluating models using cross-validation. It helps automate the time-consuming task of manually searching for the best hyperparameters and improves model performance by identifying the optimal settings that yield the best results on unseen data. By tuning the hyperparameters, GridSearchCV enables the model to achieve better performance and generalization capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4910c83-641c-477d-8ae9-4c381690b3af",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose\n",
    "one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57e78d1-0689-4170-94e4-52bd5febd732",
   "metadata": {},
   "source": [
    "####\n",
    "\n",
    "GridSearchCV and RandomizedSearchCV are both techniques used for hyperparameter tuning in machine learning. Here are the key differences between the two:\n",
    "\n",
    "GridSearchCV:\n",
    "- GridSearchCV exhaustively searches through all possible combinations of hyperparameter values specified in a predefined grid.\n",
    "- It performs a systematic grid search, evaluating the model for each combination of hyperparameters using cross-validation.\n",
    "- GridSearchCV is suitable when you have a small number of hyperparameters or when you want to explore all possible combinations thoroughly.\n",
    "- It guarantees that the optimal hyperparameters will be found within the search space, given enough computational resources and time.\n",
    "- However, GridSearchCV can be computationally expensive and time-consuming, especially when dealing with a large number of hyperparameters or a large search space.\n",
    "\n",
    "RandomizedSearchCV:\n",
    "- RandomizedSearchCV randomly samples a specified number of combinations from a predefined hyperparameter distribution.\n",
    "- It performs a randomized search, evaluating the model for randomly selected combinations of hyperparameters using cross-validation.\n",
    "- RandomizedSearchCV is suitable when you have a large number of hyperparameters or when exploring the entire search space is not feasible due to time or computational constraints.\n",
    "- It provides more flexibility by allowing you to control the number of random combinations to explore.\n",
    "- RandomizedSearchCV may not guarantee finding the absolute optimal hyperparameters, but it can often find good or near-optimal solutions with less computational effort.\n",
    "\n",
    "Choosing between GridSearchCV and RandomizedSearchCV depends on the specific requirements of your problem:\n",
    "\n",
    "- Use GridSearchCV when you have a small number of hyperparameters or when you want to exhaustively search all possible combinations. If computational resources and time are not limiting factors, and you want to be confident in finding the absolute optimal hyperparameters, GridSearchCV is a good choice.\n",
    "\n",
    "- Use RandomizedSearchCV when you have a large number of hyperparameters or a large search space and want to explore a representative subset of combinations efficiently. If you have time or computational constraints, or if finding the absolute optimal hyperparameters is less critical, RandomizedSearchCV provides a good balance between exploration and computational efficiency.\n",
    "\n",
    "In summary, GridSearchCV is suitable for thorough exploration of a small search space, while RandomizedSearchCV is more efficient for larger search spaces or when computational resources are limited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066df2e0-bfdc-4f6c-b02f-ac0d7c8ba7c8",
   "metadata": {},
   "source": [
    "####\n",
    "\n",
    "\n",
    "Q3. What is data leakage, and why is it a problem in machine learning? Provide an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7d81e2-bb68-44d3-8e9d-51810053179c",
   "metadata": {},
   "source": [
    "###\n",
    "Data leakage refers to a situation in machine learning where information from the test set or unseen data inadvertently leaks into the training process, leading to overly optimistic performance estimates and potentially misleading results. It occurs when there is unintentional mixing or use of information from the test set during the model development or feature engineering stage.\n",
    "\n",
    "Data leakage is a problem in machine learning because it violates the fundamental assumption that the model should only be exposed to the training data and should generalize well to unseen data. When data leakage occurs, the model can learn patterns or relationships that do not exist in the real world and perform poorly when applied to new data.\n",
    "\n",
    "Here's an example to illustrate data leakage:\n",
    "\n",
    "Suppose you are building a credit card fraud detection model. You have a dataset with features such as transaction amount, location, time, and a target variable indicating whether a transaction is fraudulent or not. In this scenario:\n",
    "\n",
    "1. Train-Test Split: You split the dataset into a training set (80% of the data) and a test set (20% of the data) to evaluate the model's performance on unseen data.\n",
    "\n",
    "2. Feature Engineering: During feature engineering, you accidentally include the transaction timestamp as a feature. You convert the timestamp to a categorical variable indicating the hour of the day (e.g., morning, afternoon, evening).\n",
    "\n",
    "3. Model Training: You train a machine learning model (e.g., logistic regression) on the training set using the engineered features, including the hour of the day. The model achieves high accuracy on the training set and seems to perform well during cross-validation.\n",
    "\n",
    "4. Model Evaluation: You evaluate the model on the test set and obtain surprisingly high accuracy. However, upon further investigation, you discover that the transaction timestamp in the test set is from a different time period (e.g., a different month) than the transactions in the training set. The model's high performance on the test set is not due to its ability to detect fraud but rather its unintentional dependence on the transaction timestamp, which leaked information from the test set.\n",
    "\n",
    "In this example, the inclusion of the transaction timestamp as a feature in the model introduces data leakage. The model indirectly learned the relationship between the timestamp and fraud because the timestamp is correlated with the target variable but should not have been available during model training. Consequently, the model's performance on the test set is artificially inflated, and it fails to generalize to new, unseen data.\n",
    "\n",
    "To avoid data leakage, it is crucial to ensure strict separation between the training, validation, and test sets, and to carefully review the features and preprocessing steps to ensure they are based solely on the training data. Data leakage can lead to over-optimistic results, misleading conclusions, and ineffective models, so it's essential to be mindful of this issue during the entire machine learning pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b41ec8-6f6c-4741-914b-549cb9eee465",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "Q4. How can you prevent data leakage when building a machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d1cd4c-71a8-4170-8a0a-6030ea0ce6af",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "To prevent data leakage and ensure the integrity of your machine learning model, consider the following best practices:\n",
    "\n",
    "1. Use Proper Train-Test Split: Split your dataset into a training set and a separate test set before any data preprocessing or feature engineering. This ensures that the model is trained only on the training data and evaluated solely on unseen test data.\n",
    "\n",
    "2. Feature Engineering and Preprocessing: Perform all feature engineering and preprocessing steps using only the training data. Avoid using any information from the test set during feature selection, transformation, or imputation. Apply the same preprocessing steps to both the training and test sets, but make sure the preprocessing decisions are based solely on the training set.\n",
    "\n",
    "3. Cross-Validation: Use appropriate cross-validation techniques, such as k-fold cross-validation, during model development. Cross-validation helps assess the model's performance on multiple subsets of the training data without leaking information from the test set. It provides a more robust estimate of the model's generalization performance.\n",
    "\n",
    "4. Look-Ahead Bias: Be cautious of look-ahead bias, which occurs when information from the future (unavailable at the time of prediction) is unintentionally included in the training process. Ensure that you only use information that would have been available at the time of making predictions. For example, when using time series data, do not include future data points as predictors.\n",
    "\n",
    "5. Data Pipeline Order: Pay attention to the order of operations in your data pipeline. Ensure that any transformations, scaling, or encoding of variables are applied strictly to the training data first and then separately to the test data. Avoid using summary statistics or information from the test set to compute quantities for the training set.\n",
    "\n",
    "6. Feature Selection: If you perform feature selection based on the model's performance, use nested cross-validation. In this approach, the inner cross-validation loop is used to select features, and the outer loop evaluates the model's performance. This ensures that the feature selection process is not influenced by the test set.\n",
    "\n",
    "7. Validation Set: Consider using an additional validation set (separate from the test set) for model selection and hyperparameter tuning. This set can be used to evaluate different models and select the best-performing one before final evaluation on the test set. Ensure that the validation set is kept separate throughout the entire modeling process and is not used for any training or preprocessing steps.\n",
    "\n",
    "By following these practices, you can minimize the risk of data leakage and ensure that your machine learning model learns from the appropriate information. Preventing data leakage helps maintain the model's generalization capability and ensures that its performance is accurately evaluated on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c719b67f-a70d-4602-b61c-eb06cee7dbe9",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80116346-c227-4026-8f43-61a46de8b869",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "\n",
    "A confusion matrix, also known as an error matrix, is a table that summarizes the performance of a classification model by presenting the counts of true positive (TP), true negative (TN), false positive (FP), and false negative (FN) predictions. It is commonly used in evaluating the performance of binary classification models.\n",
    "\n",
    "Here is an example of a confusion matrix:\n",
    "\n",
    "```\n",
    "                  Predicted Negative   Predicted Positive\n",
    "Actual Negative        TN                      FP\n",
    "Actual Positive        FN                      TP\n",
    "```\n",
    "\n",
    "The elements in the confusion matrix represent the following:\n",
    "\n",
    "- True Positives (TP): The number of instances correctly predicted as positive by the model.\n",
    "- True Negatives (TN): The number of instances correctly predicted as negative by the model.\n",
    "- False Positives (FP): The number of instances incorrectly predicted as positive when they are actually negative. Also known as Type I errors.\n",
    "- False Negatives (FN): The number of instances incorrectly predicted as negative when they are actually positive. Also known as Type II errors.\n",
    "\n",
    "The confusion matrix provides valuable information about the performance of a classification model:\n",
    "\n",
    "1. Accuracy: It allows you to calculate the accuracy of the model, which is the proportion of correctly classified instances over the total number of instances. Accuracy = (TP + TN) / (TP + TN + FP + FN). It provides a general overview of how well the model performs across both positive and negative classes.\n",
    "\n",
    "2. Precision: Precision is the proportion of true positive predictions out of all positive predictions. Precision = TP / (TP + FP). It measures the model's ability to correctly identify positive instances without falsely including negative instances.\n",
    "\n",
    "3. Recall (Sensitivity or True Positive Rate): Recall is the proportion of true positive predictions out of all actual positive instances. Recall = TP / (TP + FN). It captures the model's ability to correctly detect positive instances without missing them.\n",
    "\n",
    "4. Specificity (True Negative Rate): Specificity is the proportion of true negative predictions out of all actual negative instances. Specificity = TN / (TN + FP). It indicates the model's ability to correctly identify negative instances without falsely including positive instances.\n",
    "\n",
    "5. F1 Score: The F1 score is the harmonic mean of precision and recall. It provides a balanced measure of the model's performance, particularly when there is an imbalance between the positive and negative classes.\n",
    "\n",
    "By examining the confusion matrix, you can analyze the distribution of correct and incorrect predictions made by the model and derive various performance metrics to assess its effectiveness. It helps you understand the strengths and weaknesses of the model and can guide further improvements or adjustments to achieve better classification results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f2109c-3559-4ab8-880d-e010008649bd",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "Q6. Explain the difference between precision and recall in the context of a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a7f12f-b936-44ec-8ad4-4ddb5b75e8d5",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "Precision and recall are performance metrics that are derived from the confusion matrix and provide insights into the performance of a classification model, particularly in the context of imbalanced datasets.\n",
    "\n",
    "Precision:\n",
    "Precision is the proportion of true positive predictions out of all positive predictions made by the model. It quantifies how well the model correctly identifies positive instances without falsely including negative instances. Precision is computed as:\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "In other words, precision answers the question: \"Of all the instances predicted as positive, how many are actually positive?\" A high precision indicates that the model has a low rate of false positives, meaning it is more conservative in labeling instances as positive.\n",
    "\n",
    "For example, in a medical diagnosis scenario, precision represents the proportion of correctly diagnosed positive cases out of all the cases the model predicted as positive. High precision is desirable when false positive predictions are costly or could have significant consequences, such as unnecessary treatments or interventions.\n",
    "\n",
    "Recall (Sensitivity or True Positive Rate):\n",
    "Recall is the proportion of true positive predictions out of all actual positive instances in the dataset. It quantifies the model's ability to correctly detect positive instances without missing them. Recall is computed as:\n",
    "\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "In other words, recall answers the question: \"Of all the actual positive instances, how many did the model correctly identify?\" A high recall indicates that the model has a low rate of false negatives, meaning it rarely misses positive instances.\n",
    "\n",
    "In the medical diagnosis example, recall represents the proportion of correctly diagnosed positive cases out of all actual positive cases. High recall is desired when false negative predictions are costly or could lead to serious consequences, such as missing the diagnosis of a critical condition.\n",
    "\n",
    "Precision and recall are often inversely related to each other. As you increase the model's threshold for labeling instances as positive, precision typically increases while recall decreases, and vice versa. It's a trade-off between being more precise in positive predictions and capturing as many positive instances as possible.\n",
    "\n",
    "The choice between optimizing precision or recall depends on the specific requirements of the problem at hand. If false positives are more concerning, then maximizing precision is important. If false negatives are more critical, then maximizing recall is the focus. It is common to consider both precision and recall together using metrics like the F1 score, which balances the two and provides a combined evaluation of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1e30cf-85dd-434e-a241-864c0cee7310",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3868b6e2-d886-4df4-be3e-164f559f2382",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "To interpret a confusion matrix and understand the types of errors your model is making, you can examine the values in each cell of the matrix. Let's consider a binary classification confusion matrix:\n",
    "\n",
    "```\n",
    "                  Predicted Negative   Predicted Positive\n",
    "Actual Negative        TN                      FP\n",
    "Actual Positive        FN                      TP\n",
    "```\n",
    "\n",
    "Here's how you can interpret the different types of errors:\n",
    "\n",
    "1. True Negatives (TN):\n",
    "   - TN represents the instances that are truly negative and correctly predicted as negative by the model.\n",
    "   - These are the instances that the model correctly identified as negative, and there are no false alarms for these cases.\n",
    "\n",
    "2. True Positives (TP):\n",
    "   - TP represents the instances that are truly positive and correctly predicted as positive by the model.\n",
    "   - These are the instances that the model correctly identified as positive, indicating that the model successfully detected positive instances.\n",
    "\n",
    "3. False Negatives (FN):\n",
    "   - FN represents the instances that are truly positive but incorrectly predicted as negative by the model.\n",
    "   - These are the instances that the model failed to identify as positive, indicating that the model missed these positive instances. False negatives represent Type II errors.\n",
    "\n",
    "4. False Positives (FP):\n",
    "   - FP represents the instances that are truly negative but incorrectly predicted as positive by the model.\n",
    "   - These are the instances that the model falsely labeled as positive, indicating that the model made a false alarm or false positive prediction. False positives represent Type I errors.\n",
    "\n",
    "By analyzing the values in the confusion matrix, you can gain insights into the specific errors made by your model:\n",
    "\n",
    "- High FN (False Negative) Rate: If you have a significant number of FN cases, it suggests that the model has a problem with sensitivity or recall. It means the model is failing to detect positive instances correctly and is missing important cases.\n",
    "\n",
    "- High FP (False Positive) Rate: If you have a significant number of FP cases, it indicates that the model has a problem with precision. It means the model is labeling too many instances as positive when they are actually negative, leading to false alarms.\n",
    "\n",
    "- High TN (True Negative) Rate: A high TN rate suggests that the model is correctly identifying negative instances, indicating good specificity.\n",
    "\n",
    "- High TP (True Positive) Rate: A high TP rate indicates that the model is successfully detecting positive instances, reflecting good recall.\n",
    "\n",
    "Understanding the distribution of errors can guide improvements in your model. For example, if false negatives are more concerning, you might consider adjusting the model's threshold or explore additional features to improve sensitivity. Similarly, if false positives are more critical, you can focus on increasing precision by adjusting the threshold or incorporating more specific features.\n",
    "\n",
    "Overall, analyzing the confusion matrix helps you understand the strengths and weaknesses of your model's predictions and provides insights into the types of errors it is making, allowing you to make informed decisions for model refinement and performance enhancement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2f21b8-e049-484f-ad07-673b4d2666d8",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "Q8. What are some common metrics that can be derived from a confusion matrix, and how are they\n",
    "calculated?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c28fa04-7916-49ae-b2b2-8ca84ec97563",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "Several common metrics can be derived from a confusion matrix to evaluate the performance of a classification model. Here are some of the key metrics and their calculation formulas:\n",
    "\n",
    "1. Accuracy:\n",
    "   - Accuracy is the overall proportion of correct predictions made by the model.\n",
    "   - Formula: Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "2. Precision (Positive Predictive Value):\n",
    "   - Precision is the proportion of true positive predictions out of all positive predictions made by the model.\n",
    "   - Formula: Precision = TP / (TP + FP)\n",
    "\n",
    "3. Recall (Sensitivity or True Positive Rate):\n",
    "   - Recall is the proportion of true positive predictions out of all actual positive instances in the dataset.\n",
    "   - Formula: Recall = TP / (TP + FN)\n",
    "\n",
    "4. Specificity (True Negative Rate):\n",
    "   - Specificity is the proportion of true negative predictions out of all actual negative instances in the dataset.\n",
    "   - Formula: Specificity = TN / (TN + FP)\n",
    "\n",
    "5. F1 Score:\n",
    "   - The F1 score is the harmonic mean of precision and recall. It provides a balanced measure of the model's performance.\n",
    "   - Formula: F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "6. False Positive Rate (FPR):\n",
    "   - FPR is the proportion of false positive predictions out of all actual negative instances in the dataset.\n",
    "   - Formula: FPR = FP / (FP + TN)\n",
    "\n",
    "7. False Negative Rate (FNR):\n",
    "   - FNR is the proportion of false negative predictions out of all actual positive instances in the dataset.\n",
    "   - Formula: FNR = FN / (FN + TP)\n",
    "\n",
    "8. Matthews Correlation Coefficient (MCC):\n",
    "   - MCC is a correlation coefficient that measures the quality of binary classifications, taking into account all elements of the confusion matrix.\n",
    "   - Formula: MCC = (TP * TN - FP * FN) / sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
    "\n",
    "These metrics provide different perspectives on the model's performance, such as overall accuracy, precision, recall, specificity, trade-offs between precision and recall (F1 score), and the correlation between predicted and actual labels (MCC).\n",
    "\n",
    "It is important to consider the specific problem, domain, and priorities to determine which metrics are most relevant. For example, precision may be more important in situations where false positives have severe consequences, while recall may be more critical when missing positive instances (false negatives) is highly undesirable.\n",
    "\n",
    "By calculating and analyzing these metrics, you can gain a comprehensive understanding of your model's performance and make informed decisions for further model improvement or comparison with other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b792dc5b-0bc3-4871-9177-b81d64e8a194",
   "metadata": {},
   "source": [
    "Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28f6c38-4041-4330-ac6e-330e7156e73f",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "\n",
    "The accuracy of a model is closely related to the values in its confusion matrix. The confusion matrix provides a breakdown of the correct and incorrect predictions made by the model, allowing for the calculation of various performance metrics, including accuracy.\n",
    "\n",
    "Accuracy is defined as the proportion of correct predictions made by the model over the total number of predictions. It provides a general measure of how well the model performs across both positive and negative classes. The formula for accuracy is:\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "The values in the confusion matrix contribute to the calculation of accuracy as follows:\n",
    "\n",
    "- True Positives (TP) and True Negatives (TN) are the correct predictions made by the model, contributing to the numerator of the accuracy formula.\n",
    "- False Positives (FP) and False Negatives (FN) represent the incorrect predictions made by the model, which are part of the denominator of the accuracy formula.\n",
    "\n",
    "In summary, the accuracy of a model depends on the correct and incorrect predictions captured in the confusion matrix. A high number of true positives and true negatives, along with a low number of false positives and false negatives, will result in a higher accuracy score. Conversely, a significant number of false positives and false negatives will lower the accuracy of the model.\n",
    "\n",
    "However, accuracy alone may not provide a complete picture of a model's performance, especially in cases of imbalanced datasets or when the costs of different types of errors vary. It is essential to consider other metrics derived from the confusion matrix, such as precision, recall, F1 score, and specificity, to gain a more comprehensive understanding of the model's performance across different classes and error types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213a2580-1b2f-40f9-bf63-854ea07abf69",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning\n",
    "model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b529b4-63e8-47fb-84d1-99931e7f0e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
