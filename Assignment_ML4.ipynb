{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d493ff1f-fde7-4d0f-9d31-7b1d970d5d3f",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a5260f-66c4-44a8-b15e-6d7fad1ac621",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to rescale numerical features to a specific range. It transforms the feature values in such a way that they are mapped to a new range, typically between 0 and 1.\n",
    "\n",
    "The formula for Min-Max scaling is:\n",
    "\n",
    "scaled_value = (value - min_value) / (max_value - min_value)\n",
    "\n",
    "where \"value\" is the original feature value, \"min_value\" is the minimum value of the feature in the dataset, and \"max_value\" is the maximum value of the feature in the dataset.\n",
    "\n",
    "Min-Max scaling is used to ensure that all features have the same scale and do not dominate the model's learning process due to their larger magnitude. It can also help in situations where certain algorithms, such as those based on distance or gradient descent, are sensitive to the scale of features.\n",
    "\n",
    "Here's an example to illustrate the application of Min-Max scaling:\n",
    "\n",
    "Consider a dataset with a feature \"age\" that ranges from 20 to 60 years. We want to apply Min-Max scaling to rescale the feature values between 0 and 1.\n",
    "\n",
    "Original feature values:\n",
    "[20, 30, 40, 50, 60]\n",
    "\n",
    "Min-Max scaling:\n",
    "min_value = 20\n",
    "max_value = 60\n",
    "\n",
    "Scaled feature values:\n",
    "[(20-20)/(60-20), (30-20)/(60-20), (40-20)/(60-20), (50-20)/(60-20), (60-20)/(60-20)]\n",
    "= [0, 0.1667, 0.3333, 0.5000, 1]\n",
    "\n",
    "After applying Min-Max scaling, the feature values are transformed to the range between 0 and 1. This normalization ensures that all values are relative to the minimum and maximum values observed in the dataset.\n",
    "\n",
    "Note: Min-Max scaling is sensitive to outliers. If the dataset contains outliers, their presence can significantly affect the scaling process. In such cases, other scaling methods, such as standardization (Z-score scaling), may be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf26f38-6e63-4b12-9b5b-d1fef892a3a3",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07abc67a-922d-45ee-9ab1-c2102a8fd4ec",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "The Unit Vector technique, also known as Vector normalization or Normalization by Vector length, is a feature scaling method that scales the values of a feature to have a unit norm or length. It involves dividing each feature vector by its Euclidean norm, resulting in a vector with a length of 1.\n",
    "\n",
    "The formula for the Unit Vector technique is:\n",
    "\n",
    "scaled_value = value / ||vector||\n",
    "\n",
    "where \"value\" is the original feature value, and \"||vector||\" represents the Euclidean norm of the feature vector.\n",
    "\n",
    "The Unit Vector technique is primarily used when the direction or angle between feature vectors is important, rather than their magnitudes. It is commonly used in machine learning algorithms that rely on the cosine similarity between vectors.\n",
    "\n",
    "Here's an example to illustrate the application of the Unit Vector technique:\n",
    "\n",
    "Consider a dataset with two features: \"x\" and \"y.\" We want to apply the Unit Vector technique to scale the feature vectors.\n",
    "\n",
    "Original feature vectors:\n",
    "[2, 3]\n",
    "[1, 4]\n",
    "[3, 1]\n",
    "\n",
    "Unit Vector scaling:\n",
    "||vector_1|| = sqrt(2^2 + 3^2) = sqrt(13) ≈ 3.6056\n",
    "||vector_2|| = sqrt(1^2 + 4^2) = sqrt(17) ≈ 4.1231\n",
    "||vector_3|| = sqrt(3^2 + 1^2) = sqrt(10) ≈ 3.1623\n",
    "\n",
    "Scaled feature vectors:\n",
    "[2/3.6056, 3/3.6056] ≈ [0.5547, 0.8321]\n",
    "[1/4.1231, 4/4.1231] ≈ [0.2425, 0.9701]\n",
    "[3/3.1623, 1/3.1623] ≈ [0.9487, 0.3162]\n",
    "\n",
    "After applying the Unit Vector technique, each feature vector is scaled to have a unit norm or length of 1. The direction or angle between the vectors is preserved, while their magnitudes are adjusted.\n",
    "\n",
    "It's important to note that the Unit Vector technique does not preserve the original range or distribution of the feature values like Min-Max scaling. It only focuses on the direction of the vectors.\n",
    "\n",
    "The Unit Vector technique is useful in scenarios where the magnitudes of features are not relevant, and only the relative orientations or angles between vectors matter for the analysis or modeling task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8317bd-accd-47c2-86b4-d37a16156f3c",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3749fc9-35f9-4da3-a598-89315ff41afb",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique used to transform a high-dimensional dataset into a lower-dimensional space while retaining the most important information or patterns in the data. It achieves this by identifying the principal components, which are new uncorrelated variables that are linear combinations of the original features.\n",
    "\n",
    "The main steps involved in PCA are as follows:\n",
    "\n",
    "1. Standardize the data: If necessary, standardize the features by subtracting the mean and dividing by the standard deviation. This step ensures that all features have a similar scale.\n",
    "\n",
    "2. Compute the covariance matrix: Calculate the covariance matrix of the standardized data. The covariance matrix provides information about the relationships and variances among the features.\n",
    "\n",
    "3. Compute the eigenvectors and eigenvalues: Perform an eigendecomposition of the covariance matrix to obtain the eigenvectors and eigenvalues. The eigenvectors represent the principal components, and the corresponding eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "4. Select the principal components: Sort the eigenvectors based on their corresponding eigenvalues in decreasing order. Choose the top k eigenvectors that explain the most variance (where k is the desired number of dimensions in the reduced space).\n",
    "\n",
    "5. Project the data onto the new space: Use the selected eigenvectors to transform the standardized data into the reduced-dimensional space. This step involves taking the dot product of the data and the eigenvectors to obtain the transformed data.\n",
    "\n",
    "Here's an example to illustrate the application of PCA:\n",
    "\n",
    "Consider a dataset with three features: \"x1,\" \"x2,\" and \"x3.\" We want to apply PCA to reduce the dimensionality to two principal components.\n",
    "\n",
    "Original dataset:\n",
    "[2, 4, 5]\n",
    "[3, 6, 2]\n",
    "[5, 7, 8]\n",
    "[4, 2, 3]\n",
    "\n",
    "Step 1: Standardize the data (if necessary).\n",
    "\n",
    "Step 2: Compute the covariance matrix.\n",
    "\n",
    "Covariance matrix:\n",
    "[[2.6667, 2.3333, 2.6667],\n",
    " [2.3333, 4.6667, 1.3333],\n",
    " [2.6667, 1.3333, 6.6667]]\n",
    "\n",
    "Step 3: Compute the eigenvectors and eigenvalues.\n",
    "\n",
    "Eigenvectors:\n",
    "[-0.4558, -0.7868, 0.4192]\n",
    "[-0.5331, 0.5112, 0.6734]\n",
    "[-0.7137, 0.3446, -0.6103]\n",
    "\n",
    "Eigenvalues:\n",
    "[9.7624, 3.5375, 1.0341]\n",
    "\n",
    "Step 4: Select the principal components.\n",
    "\n",
    "Since we want to reduce the dimensionality to two, we choose the top two eigenvectors.\n",
    "\n",
    "Selected eigenvectors:\n",
    "[-0.4558, -0.7868, 0.4192]\n",
    "[-0.5331, 0.5112, 0.6734]\n",
    "\n",
    "Step 5: Project the data onto the new space.\n",
    "\n",
    "Transformed data:\n",
    "[-4.6855, 0.3194]\n",
    "[-6.4719, 0.5886]\n",
    "[-8.1379, -1.1612]\n",
    "[-4.6899, 0.2531]\n",
    "\n",
    "After applying PCA, the original three-dimensional dataset is transformed into a two-dimensional space defined by the principal components. The transformed data captures the most important information in the original dataset while reducing its dimensionality.\n",
    "\n",
    "PCA is widely used in various applications, such as image processing, genetics, finance, and pattern recognition, where high-dimensional data need to be analyzed or visualized efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6c09f8-ad6a-43c1-ae63-81e1b3042f53",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94445017-6764-4f01-8c9c-ee17a8753e3d",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "\n",
    "PCA and feature extraction are closely related concepts, with PCA being a specific method used for feature extraction.\n",
    "\n",
    "Feature extraction refers to the process of transforming raw data into a lower-dimensional representation while retaining the most relevant information or patterns. The goal is to create a new set of features that capture the essential characteristics of the original data.\n",
    "\n",
    "PCA can be used as a feature extraction technique by identifying the principal components, which are linear combinations of the original features. These principal components represent the most important information in the data and can be used as the new features.\n",
    "\n",
    "Here's an example to illustrate how PCA can be used for feature extraction:\n",
    "\n",
    "Consider a dataset with four features: \"x1,\" \"x2,\" \"x3,\" and \"x4.\" We want to use PCA to extract two principal components as new features.\n",
    "\n",
    "Original dataset:\n",
    "[2, 4, 5, 3]\n",
    "[3, 6, 2, 5]\n",
    "[5, 7, 8, 4]\n",
    "[4, 2, 3, 6]\n",
    "\n",
    "Step 1: Standardize the data (if necessary).\n",
    "\n",
    "Step 2: Compute the covariance matrix.\n",
    "\n",
    "Covariance matrix:\n",
    "[[2.6667, 1.3333, 1.6667, 0.6667],\n",
    " [1.3333, 4.6667, 1.3333, 2.6667],\n",
    " [1.6667, 1.3333, 5.6667, 2.3333],\n",
    " [0.6667, 2.6667, 2.3333, 3.6667]]\n",
    "\n",
    "Step 3: Compute the eigenvectors and eigenvalues.\n",
    "\n",
    "Eigenvectors:\n",
    "[-0.2762, -0.7148, 0.5809, -0.3150]\n",
    "[-0.4980, -0.0405, -0.2377, -0.8347]\n",
    "[-0.6189, 0.4039, 0.3932, 0.5430]\n",
    "[-0.5222, 0.5684, -0.6664, 0.1180]\n",
    "\n",
    "Eigenvalues:\n",
    "[9.5860, 2.5026, 1.5345, 0.1887]\n",
    "\n",
    "Step 4: Select the principal components.\n",
    "\n",
    "Since we want to extract two principal components, we choose the top two eigenvectors.\n",
    "\n",
    "Selected eigenvectors:\n",
    "[-0.2762, -0.7148, 0.5809, -0.3150]\n",
    "[-0.4980, -0.0405, -0.2377, -0.8347]\n",
    "\n",
    "Step 5: Project the data onto the new feature space.\n",
    "\n",
    "Transformed data:\n",
    "[-2.4305, -0.3659]\n",
    "[-4.1023, 0.1355]\n",
    "[-7.0887, 0.4566]\n",
    "[-2.5302, -1.5476]\n",
    "\n",
    "After applying PCA, the original four-dimensional dataset is transformed into a two-dimensional feature space defined by the principal components. The transformed data represents a lower-dimensional representation of the original data, capturing the most important information or patterns.\n",
    "\n",
    "The extracted principal components can be used as the new features for subsequent analysis or modeling tasks. They often provide a more compact representation of the data while preserving the essential characteristics.\n",
    "\n",
    "Feature extraction using PCA can be beneficial in reducing the dimensionality of high-dimensional datasets, removing irrelevant or redundant features, and improving computational efficiency in subsequent analyses or modeling tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c84d770-6644-4021-b6a8-5dd7556cd5d9",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d423206-fcc1-4feb-88b4-aa64dcbe45e1",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "In the context of building a recommendation system for a food delivery service, Min-Max scaling can be used to preprocess the features such as price, rating, and delivery time. Min-Max scaling is a feature scaling technique that rescales the values of a feature to a specific range, typically between 0 and 1.\n",
    "\n",
    "Here's how you can use Min-Max scaling to preprocess the data:\n",
    "\n",
    "1. Determine the range: Identify the minimum and maximum values of each feature. For example, for the \"price\" feature, find the minimum and maximum prices in the dataset.\n",
    "\n",
    "2. Define the desired range: Decide on the desired range to which you want to rescale the values. In this case, you can choose the range between 0 and 1.\n",
    "\n",
    "3. Apply the Min-Max scaling formula: For each feature value, use the Min-Max scaling formula to rescale the values to the desired range:\n",
    "\n",
    "   scaled_value = (value - min_value) / (max_value - min_value)\n",
    "\n",
    "   where \"value\" is the original feature value, \"min_value\" is the minimum value of the feature, and \"max_value\" is the maximum value of the feature.\n",
    "\n",
    "4. Perform the scaling: Apply the Min-Max scaling formula to each feature in the dataset. This will transform the values of the features to the range between 0 and 1.\n",
    "\n",
    "The benefits of using Min-Max scaling in this context are as follows:\n",
    "\n",
    "1. Normalization: Min-Max scaling ensures that all the features are on the same scale, preventing any particular feature from dominating the recommendation process due to its larger values.\n",
    "\n",
    "2. Interpretable values: The rescaled values between 0 and 1 are easy to interpret, with 0 representing the minimum value and 1 representing the maximum value.\n",
    "\n",
    "For example, let's say you have the following data for three food items in the dataset:\n",
    "\n",
    "Item 1: price = $10, rating = 4.5, delivery time = 30 minutes\n",
    "Item 2: price = $15, rating = 3.7, delivery time = 45 minutes\n",
    "Item 3: price = $20, rating = 4.2, delivery time = 50 minutes\n",
    "\n",
    "Step 1: Determine the range:\n",
    "- Price: Minimum = $10, Maximum = $20\n",
    "- Rating: Minimum = 3.7, Maximum = 4.5\n",
    "- Delivery time: Minimum = 30 minutes, Maximum = 50 minutes\n",
    "\n",
    "Step 2: Define the desired range: 0 to 1\n",
    "\n",
    "Step 3: Apply the Min-Max scaling formula:\n",
    "- Price of Item 1: (10 - 10) / (20 - 10) = 0\n",
    "- Rating of Item 1: (4.5 - 3.7) / (4.5 - 3.7) = 1\n",
    "- Delivery time of Item 1: (30 - 30) / (50 - 30) = 0\n",
    "\n",
    "Repeat the same steps for the remaining items.\n",
    "\n",
    "After applying Min-Max scaling, you will have the rescaled values for each feature, which can be used as input for the recommendation system. The scaled values ensure that all the features are on the same scale, allowing for a fair comparison and accurate recommendations based on user preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5886fc-89ca-42e2-ab9c-07514fd7705b",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfb7f90-0a17-43e3-8799-0b77dfaa88d9",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "In the project to predict stock prices with a dataset containing multiple features, PCA (Principal Component Analysis) can be used to reduce the dimensionality of the dataset. Dimensionality reduction with PCA involves transforming the original high-dimensional features into a lower-dimensional space while preserving the most important information or patterns in the data.\n",
    "\n",
    "Here's how you can use PCA to reduce the dimensionality of the dataset in the context of predicting stock prices:\n",
    "\n",
    "1. Data preparation: Prepare the dataset by collecting relevant features related to company financial data and market trends. Ensure that the data is preprocessed, such as handling missing values, scaling the features, and dealing with any necessary feature engineering.\n",
    "\n",
    "2. Standardize the data: Since PCA is affected by the scale of the features, it is essential to standardize the data before applying PCA. Standardization involves transforming the features to have zero mean and unit variance.\n",
    "\n",
    "3. Apply PCA: Use the PCA algorithm to perform dimensionality reduction on the standardized dataset. PCA will calculate the principal components, which are linear combinations of the original features that capture the most significant variance in the data.\n",
    "\n",
    "4. Determine the number of components: Analyze the explained variance ratio or scree plot to understand how much variance each principal component explains. Based on the desired level of dimensionality reduction, decide on the number of principal components to retain. You can consider keeping the components that explain a significant portion of the total variance, such as 90% or 95%.\n",
    "\n",
    "5. Transform the data: Transform the standardized data using the selected principal components. This transformation maps the original high-dimensional data into a lower-dimensional space defined by the principal components.\n",
    "\n",
    "6. Use the transformed data for modeling: The transformed data, consisting of the reduced number of features derived from the principal components, can be used as input for the stock price prediction model. You can apply various machine learning algorithms or time series forecasting techniques to train the model and make predictions.\n",
    "\n",
    "The benefits of using PCA for dimensionality reduction in the context of predicting stock prices are as follows:\n",
    "\n",
    "1. Reducing complexity: By reducing the number of features, PCA simplifies the modeling process and makes it computationally efficient.\n",
    "\n",
    "2. Removing redundant information: PCA identifies the most significant patterns in the data, capturing the essential information while discarding redundant or less informative features.\n",
    "\n",
    "3. Dealing with multicollinearity: If the original features are highly correlated with each other, PCA can help overcome multicollinearity issues by transforming them into orthogonal principal components.\n",
    "\n",
    "4. Improved model performance: By reducing dimensionality, PCA can help improve the model's performance by focusing on the most important features and avoiding overfitting.\n",
    "\n",
    "Note that while PCA can reduce dimensionality, it may also result in a loss of interpretability as the transformed features are combinations of the original features. It's important to strike a balance between dimensionality reduction and preserving the meaningful interpretability of the features in the specific domain, such as stock price prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe224f27-b1ec-4ad0-a2c0-22c954611ca1",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e0057b-4f84-4dee-9a61-5cd7b3301465",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "To perform Min-Max scaling on the dataset [1, 5, 10, 15, 20] and transform the values to a range of -1 to 1, we need to follow these steps:\n",
    "\n",
    "1. Determine the minimum and maximum values in the dataset:\n",
    "   Minimum value: 1\n",
    "   Maximum value: 20\n",
    "\n",
    "2. Define the desired range: -1 to 1\n",
    "\n",
    "3. Apply the Min-Max scaling formula to each value in the dataset:\n",
    "   scaled_value = (value - min_value) / (max_value - min_value)\n",
    "\n",
    "   Let's calculate the scaled values:\n",
    "\n",
    "   For 1:\n",
    "   scaled_value = (1 - 1) / (20 - 1) = 0 / 19 = 0\n",
    "\n",
    "   For 5:\n",
    "   scaled_value = (5 - 1) / (20 - 1) = 4 / 19 ≈ 0.2105\n",
    "\n",
    "   For 10:\n",
    "   scaled_value = (10 - 1) / (20 - 1) = 9 / 19 ≈ 0.4737\n",
    "\n",
    "   For 15:\n",
    "   scaled_value = (15 - 1) / (20 - 1) = 14 / 19 ≈ 0.7368\n",
    "\n",
    "   For 20:\n",
    "   scaled_value = (20 - 1) / (20 - 1) = 19 / 19 = 1\n",
    "\n",
    "4. Rescale the values to the desired range of -1 to 1:\n",
    "   scaled_value = 2 * scaled_value - 1\n",
    "\n",
    "   Rescaled values:\n",
    "\n",
    "   For 1: 2 * 0 - 1 = -1\n",
    "   For 5: 2 * 0.2105 - 1 ≈ -0.5789\n",
    "   For 10: 2 * 0.4737 - 1 ≈ -0.0526\n",
    "   For 15: 2 * 0.7368 - 1 ≈ 0.4737\n",
    "   For 20: 2 * 1 - 1 = 1\n",
    "\n",
    "Therefore, after applying Min-Max scaling, the dataset [1, 5, 10, 15, 20] is transformed to the range of -1 to 1 as follows:\n",
    "[-1, -0.5789, -0.0526, 0.4737, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeb2072-9470-4b46-8b5e-028bacb2e7fd",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2875c9dd-c9ca-484a-b874-141040fa4768",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "To perform feature extraction using PCA on a dataset with features [height, weight, age, gender, blood pressure], we need to follow these steps:\n",
    "\n",
    "1. Prepare the data: Ensure that the dataset is properly preprocessed, including handling missing values, encoding categorical variables (like gender), and scaling the numerical features if necessary.\n",
    "\n",
    "2. Standardize the data: Standardize the features to have zero mean and unit variance. This step is important to ensure that features with larger scales do not dominate the PCA analysis.\n",
    "\n",
    "3. Apply PCA: Use the PCA algorithm to calculate the principal components. The number of principal components obtained will be equal to the number of original features.\n",
    "\n",
    "4. Determine the explained variance ratio: Analyze the explained variance ratio to understand how much variance is explained by each principal component. The explained variance ratio indicates the proportion of variance in the original data that is captured by each principal component.\n",
    "\n",
    "5. Decide on the number of principal components to retain: Based on the explained variance ratio, decide how many principal components to retain. A common approach is to choose the number of components that capture a significant portion of the total variance, such as 90% or 95%. Retaining fewer components reduces the dimensionality of the data while still preserving most of the important information.\n",
    "\n",
    "The choice of the number of principal components to retain depends on the specific dataset and the desired trade-off between dimensionality reduction and information retention. In this case, where we have five original features [height, weight, age, gender, blood pressure], we need to analyze the explained variance ratio to make an informed decision.\n",
    "\n",
    "Let's assume that after applying PCA, we obtain the following explained variance ratio for the principal components:\n",
    "\n",
    "Principal Component 1: 0.6\n",
    "Principal Component 2: 0.3\n",
    "Principal Component 3: 0.08\n",
    "Principal Component 4: 0.015\n",
    "Principal Component 5: 0.005\n",
    "\n",
    "In this example, the explained variance ratio suggests that the first two principal components capture a significant portion of the variance in the data (0.6 + 0.3 = 0.9 or 90%). Retaining these two components would reduce the dimensionality from five original features to two principal components, while still preserving most of the important information.\n",
    "\n",
    "Therefore, in this case, I would choose to retain two principal components because they explain a significant portion of the total variance in the dataset. Retaining fewer components simplifies the data representation and visualization while still retaining the most important patterns and information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ed21a2-7ea7-4e2f-b585-93aa967b2aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
